{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0yfr23q2L+7GJyGyYPDrV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rlhaviland/natural_language_practice/blob/main/Discover_Insights_into_Classic_Texts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = open(\"dorian_gray.txt\",encoding='utf-8').read().lower()\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt_tab') # Download 'punkt_tab' resource\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d_njbjPeAHNu",
        "outputId": "0f8f6a35-d197-4945-ff5b-0ad9f0c8ba35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save any word tokenized sentence in word_tokenized_text to a variable named single_word_tokenized_sentence. Print single_word_tokenized_sentence as a check to visualize what you have done so far!"
      ],
      "metadata": {
        "id": "B8fV-00mCC1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_word_tokenized_sentence = word_tokenized_text[100]  # Access the first sentence\n",
        "print(single_word_tokenized_sentence)  # Print the sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eT5lDPM6Aagv",
        "outputId": "f4c17800-7aa1-435b-eea5-c0e53dfd607b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', 'seems', 'to', 'be', 'the', 'one', 'thing', 'that', 'can', 'make', 'modern', 'life', 'mysterious', 'or', 'marvellous', 'to', 'us', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Next you can part-of-speech tag each sentence to allow for syntax parsing! Begin by creating a list named pos_tagged_text that will hold each part-of-speech tagged sentence from the novel."
      ],
      "metadata": {
        "id": "3dZuLOTIB-NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create an empty list to store\n",
        "pos_tagged_text = []\n",
        "#iterate through each sentences and POS-tag it\n",
        "for sentence in sentences:\n",
        "  words = word_tokenize(sentence)\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  pos_tagged_text.append(pos_tags)"
      ],
      "metadata": {
        "id": "uwuwVCq8BqUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Loop through each word tokenized sentence in word_tokenized_text and part-of-speech tag each sentence using nltkâ€˜s pos_tag() function. Append the result to pos_tagged_text."
      ],
      "metadata": {
        "id": "vOdrLOyNCRyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_text = []  # Initialize an empty list to store tagged sentences\n",
        "\n",
        "for sentence in word_tokenized_text:\n",
        "    pos_tags = nltk.pos_tag(sentence)  # POS tag the current sentence\n",
        "    pos_tagged_text.append(pos_tags)  # Append the tagged sentence to the list"
      ],
      "metadata": {
        "id": "aHhkIfvwB7tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Save any part-of-speech tagged sentence in pos_tagged_text to a variable named single_pos_sentence. Print single_pos_sentence as a check to visualize what you have done so far!"
      ],
      "metadata": {
        "id": "EnceYKmwCfgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "single_pos_sentence = pos_tagged_text[0]  # Access the first POS-tagged sentence\n",
        "print(single_pos_sentence)  # Print the sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzXazzR7Chrr",
        "outputId": "1dfa4ec4-c8ac-4477-ca92-a2e81aea195c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'DT'), ('picture', 'NN'), ('of', 'IN'), ('dorian', 'JJ'), ('gray', 'JJ'), ('by', 'IN'), ('oscar', 'NN'), ('wilde', 'NN'), ('the', 'DT'), ('preface', 'NN'), ('the', 'DT'), ('artist', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('creator', 'NN'), ('of', 'IN'), ('beautiful', 'JJ'), ('things', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chuck Sentences\n",
        "\n",
        "Now that you have part-of-speech tagged your text, you can move on to syntax parsing!\n",
        "\n",
        "Begin by defining a piece of chunk grammar np_chunk_grammar that will chunk a noun phrase. Remember, a noun phrase consists of an optional determiner DT, followed by any number of adjectives JJ, followed by a noun NN."
      ],
      "metadata": {
        "id": "OKr1oCmvClmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_chunk_grammar = r\"\"\"\n",
        "  NP: {<DT>?<JJ>*<NN>}   # Chunk optional determiner, adjectives, and noun\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "afGziA3ZCoyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Create a nltk RegexpParser object named np_chunk_parser using the noun phrase chunk grammar you defined as an argument."
      ],
      "metadata": {
        "id": "7kO9SIBgCxQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "np_chunk_parser = RegexpParser(np_chunk_grammar)"
      ],
      "metadata": {
        "id": "SXcvMy2NCx-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define a piece of chunk grammar named vp_chunk_grammar that will chunk a verb phrase of the following form: noun phrase, followed by a verb VB, followed by an optional adverb RB."
      ],
      "metadata": {
        "id": "ZXo5s8SNC7dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vp_chunk_grammar = r\"\"\"\n",
        "     VP: {<VB.*><RB.?>?}    # Chunk verb (any tense) and optional adverb (any type)\n",
        "     \"\"\""
      ],
      "metadata": {
        "id": "cLV4H03bDDE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a nltk RegexpParser object named vp_chunk_parser using the verb phrase chunk grammar you defined as an argument."
      ],
      "metadata": {
        "id": "xNo8hDUVDG62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "vp_chunk_parser = RegexpParser(vp_chunk_grammar)"
      ],
      "metadata": {
        "id": "BtOOGnZeDHq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Create two empty lists np_chunked_text and vp_chunked_text that will hold the chunked sentences from your text."
      ],
      "metadata": {
        "id": "RhOu6vtVDNcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_chunked_text = []\n",
        "vp_chunked_text = []"
      ],
      "metadata": {
        "id": "RuImzoNlDN7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop through each part-of-speech tagged sentence in pos_tagged_text and noun phrase chunk each sentence using your RegexpParserâ€˜s .parse() method. Append the result to np_chunked_text."
      ],
      "metadata": {
        "id": "Ax7w0XawDXOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in pos_tagged_text:\n",
        "  chunked_sentence = np_chunk_parser.parse(sentence)  # Chunk the sentence\n",
        "  np_chunked_text.append(chunked_sentence)  # Append to np_chunked_text"
      ],
      "metadata": {
        "id": "_h_z32fkDelT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within the same loop you defined in the previous task, verb phrase chunk each part-of-speech tagged sentence using your RegexpParserâ€˜s .parse() method. Append the result to vp_chunked_text."
      ],
      "metadata": {
        "id": "Y9EJljWrDhjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in pos_tagged_text:\n",
        "    np_chunked_sentence = np_chunk_parser.parse(sentence)\n",
        "    np_chunked_text.append(np_chunked_sentence)\n",
        "\n",
        "    vp_chunked_sentence = vp_chunk_parser.parse(sentence)  # Verb phrase chunking\n",
        "    vp_chunked_text.append(vp_chunked_sentence)  # Append to vp_chunked_text"
      ],
      "metadata": {
        "id": "GCbba3BRDlva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze Chunks\n",
        "\n",
        "Now that you have chunked your novel, you can analyze the chunk frequencies to gain insights!\n",
        "\n",
        "Define chunk counter and call np_chunk_counter() with np_chunked_text as an argument and save the result to a variable named most_common_np_chunks. Print most_common_np_chunks. What sticks out to you about the most common noun phrase chunks? Are you surprised by anything? Open the hint to see our analysis.\n",
        "\n",
        "Want to see how np_chunk_counter() works? Use the file navigator to open chunk_counters.py and inspect np_chunk_counter()."
      ],
      "metadata": {
        "id": "XBOgRStbDt-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#define chunk counter\n",
        "from collections import Counter\n",
        "from nltk import Tree\n",
        "\n",
        "def np_chunk_counter(chunked_sentences):\n",
        "    \"\"\"\n",
        "    Extracts and counts noun phrase (NP) chunks from a list of chunked sentences.\n",
        "\n",
        "    Args:\n",
        "        chunked_sentences: A list of chunked sentences (nltk.Tree objects).\n",
        "\n",
        "    Returns:\n",
        "        A list of the 30 most common NP chunks and their frequencies.\n",
        "    \"\"\"\n",
        "    # Create a list to store all NP chunks\n",
        "    np_chunks = []\n",
        "\n",
        "    # Iterate through each chunked sentence and extract NP chunks\n",
        "    for chunked_sentence in chunked_sentences:\n",
        "        for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'NP'):\n",
        "            np_chunks.append(tuple(subtree))\n",
        "\n",
        "    # Count the frequency of each NP chunk\n",
        "    chunk_counter = Counter(np_chunks)\n",
        "\n",
        "    # Return the 30 most common NP chunks\n",
        "    return chunk_counter.most_common(30)\n",
        "\n",
        "# Now you can call the function:\n",
        "most_common_np_chunks = np_chunk_counter(np_chunked_text)\n",
        "print(most_common_np_chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_hWNsobElVZ",
        "outputId": "3fbf24ce-c828-44c3-85f6-1a831d23db58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[((('i', 'NN'),), 1928), ((('henry', 'NN'),), 404), ((('lord', 'NN'),), 394), ((('life', 'NN'),), 342), ((('harry', 'NN'),), 272), ((('dorian', 'JJ'), ('gray', 'NN')), 254), ((('something', 'NN'),), 252), ((('nothing', 'NN'),), 186), ((('basil', 'NN'),), 168), ((('the', 'DT'), ('world', 'NN')), 140), ((('everything', 'NN'),), 138), ((('anything', 'NN'),), 136), ((('hallward', 'NN'),), 136), ((('the', 'DT'), ('man', 'NN')), 122), ((('the', 'DT'), ('room', 'NN')), 120), ((('face', 'NN'),), 114), ((('the', 'DT'), ('door', 'NN')), 112), ((('love', 'NN'),), 110), ((('art', 'NN'),), 104), ((('course', 'NN'),), 102), ((('the', 'DT'), ('picture', 'NN')), 92), ((('the', 'DT'), ('lad', 'NN')), 92), ((('head', 'NN'),), 88), ((('round', 'NN'),), 88), ((('hand', 'NN'),), 88), ((('the', 'DT'), ('table', 'NN')), 80), ((('sibyl', 'NN'),), 80), ((('the', 'DT'), ('painter', 'NN')), 76), ((('sir', 'NN'),), 76), ((('a', 'DT'), ('moment', 'NN')), 76)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Create a function vp_chunk_counter()"
      ],
      "metadata": {
        "id": "k-wRBZHTE2Hw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Call vp_chunk_counter() with vp_chunked_text as an argument and save the result to a variable named most_common_vp_chunks. Print most_common_vp_chunks. What sticks out to you about the most common verb phrase chunks? Are you surprised by anything? Open the hint to see our analysis.\n",
        "\n",
        "Want to see how vp_chunk_counter() works? Use the file navigator to open chunk_counters.py and inspect vp_chunk_counter()."
      ],
      "metadata": {
        "id": "6LGygGDNKEM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vp_chunk_counter(chunked_sentences):\n",
        "  vp_chunks = []\n",
        "  for chunked_sentence in chunked_sentences:\n",
        "    for subtree in chunked_sentence.subtrees(filter=lambda t: t.label() == 'VP'):\n",
        "      vp_chunks.append(tuple(subtree))\n",
        "\n",
        "  chunk_counter = Counter(vp_chunks)\n",
        "  return chunk_counter.most_common(30)\n",
        "\n",
        "most_common_vp_chunks = vp_chunk_counter(vp_chunked_text)\n",
        "print(most_common_vp_chunks)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8ohxCBKKA2G",
        "outputId": "8d3943ed-1644-43d9-dddf-81edd91b521c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    }
  ]
}